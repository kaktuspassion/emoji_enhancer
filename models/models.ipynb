{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a primer on how some commonly used static embeddings perform on similarity tasks.\n",
        "\n",
        "**Word embeddings** include:\n",
        "- SpaCy embeddings\n",
        "- Word2Vec\n",
        "- GloVe\n",
        "- FastText\n",
        "\n",
        "**Emoji embeddings** include:\n",
        "- emoji2vec\n",
        "- emojional\n",
        "\n",
        "Both can be used along with word2vec-google-news-300\n",
        "\n",
        "**Basic similarity tasks** including:\n",
        "- find top 10 most similar tokens (cosine similarity by default)\n",
        "- find top 10 most similar emojis (cosine similarity by default)\n",
        "- find top 10 most similar tokens (euclidean distance)\n",
        "- find top 10 most similar emojis (euclidean distance)"
      ],
      "metadata": {
        "id": "I4FGyxGTJypR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "7z_-MIuoCEc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4117d42-78c3-457c-a87e-d458c129ecfb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np # for similarity calculation"
      ],
      "metadata": {
        "id": "KaTqZ0qvS-px"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SpaCy"
      ],
      "metadata": {
        "id": "QjsGIqA9CBMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download spaCy\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvZFkTJl-rZh",
        "outputId": "b2b6f6e2-31f0-4c25-c268-69b9f818340f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "lemmatizer = nlp.get_pipe(\"lemmatizer\")"
      ],
      "metadata": {
        "id": "z8IuVzFu-U5-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between 2 tokens\n",
        "word1 = nlp(\"happy\")\n",
        "word2 = nlp(\"joyful\")\n",
        "\n",
        "similarity = word1.similarity(word2)\n",
        "print(\"Similarity:\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sR2AWAGAMHE",
        "outputId": "9ab50cfc-5d4b-4da8-870b-c6baff4fa42b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity: 0.5075500964207872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find most similary phrase to a word\n",
        "phrases = [\"joyful time\", \"sad moment\", \"pleased atmosphere\", \"delightful experience\", \"content feeling\"]\n",
        "\n",
        "happy_doc = nlp(\"happy\")\n",
        "similarity_scores = {}\n",
        "\n",
        "for phrase in phrases:\n",
        "    phrase_doc = nlp(phrase)\n",
        "    similarity = happy_doc.similarity(phrase_doc)\n",
        "    similarity_scores[phrase] = similarity\n",
        "\n",
        "most_similar_phrase = max(similarity_scores, key=similarity_scores.get)\n",
        "print(\"Most similar phrase to 'happy':\", most_similar_phrase)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnZPO1qcQFjL",
        "outputId": "e1ed5108-ad63-4d33-9746-c795f22945f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar phrase to 'happy': pleased atmosphere\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Find top 10 most similar tokens (cosine similarity by default)\n",
        "word = nlp(\"run\")\n",
        "\n",
        "similar_words = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)[:11]\n",
        "\n",
        "for token in similar_words:\n",
        "    if token.text != \"run\":\n",
        "        print(token.text, token.similarity(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzSVjf30_R_U",
        "outputId": "a7d08e3f-4875-4235-ca25-6a5a9f904cc5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time 0.2951683405947487\n",
            "Ought 0.2727500607475427\n",
            "O‚Äôclock 0.2568140671468667\n",
            "got 0.25313470998735677\n",
            "there 0.2513292723653763\n",
            "goin‚Äô 0.2351591379197318\n",
            "O'clock 0.23487492170225424\n",
            "goin 0.22937445520425834\n",
            "goin' 0.22069260268271804\n",
            "have 0.2196272277277968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-581ee2677dc8>:4: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  similar_words = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)[:11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Find top 10 most similar tokens (euclidean distance)\n",
        "happy_vector = nlp(\"run\").vector\n",
        "\n",
        "# calculate euclidean distance with all tokens\n",
        "distances = []\n",
        "for word in nlp.vocab:\n",
        "    if word.has_vector and word.is_alpha:\n",
        "        dist = np.linalg.norm(happy_vector - word.vector)\n",
        "        distances.append((word.text, dist))\n",
        "\n",
        "# sort and select top 10\n",
        "top10_similar_words = sorted(distances, key=lambda x: x[1])[1:11]\n",
        "for word, dist in top10_similar_words:\n",
        "    print(f\"{word}: {dist}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwgYtPBcIH4J",
        "outputId": "bafdd7e6-6145-44f7-b1d9-e4030fd0ed95"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ought: 68.85926055908203\n",
            "goin: 70.31033325195312\n",
            "Kans: 71.58335876464844\n",
            "Adm: 72.10071563720703\n",
            "Id: 72.11503601074219\n",
            "Wis: 72.27690124511719\n",
            "Doin: 72.83971405029297\n",
            "Nev: 72.84615325927734\n",
            "Coz: 72.89249420166016\n",
            "Ariz: 72.98558807373047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "3iN3uUDxDJD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access word2vec Google-news-300\n",
        "import gensim.downloader as api\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "dTlPagpRDP4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222e2d8d-25ec-438e-ae70-7260b82ef91f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between 2 tokens\n",
        "if \"happy\" in w2v_model.key_to_index and \"joyful\" in w2v_model.key_to_index:\n",
        "    similarity = w2v_model.similarity(\"happy\", \"joyful\")\n",
        "    print(\"Similarity:\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzXRlQs0AV0J",
        "outputId": "7558e2c1-6c86-4b55-d1ab-d47b897b9418"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity: 0.42381963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Find top 10 most similar tokens (cosine similarity by default)\n",
        "if \"run\" in w2v_model.key_to_index:\n",
        "    similar_words = w2v_model.most_similar(\"run\", topn=10)\n",
        "    for word, similarity in similar_words:\n",
        "        print(word, similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms6i_hA7AMwN",
        "outputId": "c5ac1464-ca1b-4e34-f146-d17de8c08c73"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runs 0.6569936275482178\n",
            "running 0.6062965989112854\n",
            "drive 0.4834049642086029\n",
            "ran 0.4764978289604187\n",
            "scamper 0.46932122111320496\n",
            "tworun_double 0.46402227878570557\n",
            "go 0.4631645083427429\n",
            "twoout 0.45749351382255554\n",
            "walk 0.45697975158691406\n",
            "Mark_Grudzielanek_singled 0.4565179646015167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Find top 10 most similar tokens (euclidean distance)\n",
        "# check accessibility and get the embedding\n",
        "if 'run' in w2v_model.key_to_index:\n",
        "    happy_vector = w2v_model['run']\n",
        "else:\n",
        "    raise ValueError(\"The word is not in the model's vocabulary.\")\n",
        "\n",
        "# calculate euclidean distance with all tokens\n",
        "distances = []\n",
        "for word in w2v_model.key_to_index.keys():\n",
        "    word_vector = w2v_model[word]\n",
        "    dist = np.linalg.norm(happy_vector - word_vector)\n",
        "    distances.append((word, dist))\n",
        "\n",
        "# sort and select top 10\n",
        "top10_similar_words = sorted(distances, key=lambda x: x[1])[1:11]\n",
        "\n",
        "for word, dist in top10_similar_words:\n",
        "    print(f\"{word}: {dist}\")\n"
      ],
      "metadata": {
        "id": "mAdual6oHISR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ec4126-1b2f-490e-f584-8a71bd938496"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batterymate_Miguel_Olivo: 1.7986350059509277\n",
            "Peter_Bourjos_tripled: 1.8141846656799316\n",
            "Reliever_Macay_McBride: 1.8192209005355835\n",
            "Alec_Lowrey: 1.824215292930603\n",
            "Earnest_Rhone: 1.8242930173873901\n",
            "Melisa_Koutz: 1.8245304822921753\n",
            "Joey_Swatfager: 1.8273006677627563\n",
            "Miguel_Cabrera_belted: 1.829398274421692\n",
            "Nate_Rolison: 1.8309024572372437\n",
            "Austin_Kearns_grounded: 1.8338943719863892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe"
      ],
      "metadata": {
        "id": "-HE1xDnLB7t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe model (this will download the model if not already cached locally)\n",
        "glove_model = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sJZJ_qEBpw1",
        "outputId": "ad9b5ef7-70e1-4e41-d322-02582262445b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between 2 tokens\n",
        "if 'happy' in glove_model.key_to_index and 'joyful' in glove_model.key_to_index:\n",
        "    similarity = glove_model.similarity('happy', 'joyful')\n",
        "    print(f\"Similarity between 'happy' and 'joyful': {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpMjnT_rCZM7",
        "outputId": "dfed443a-2f42-4f9f-b8b1-10e2d81115fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'happy' and 'joyful': 0.4751318395137787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Find top 10 most similar tokens (cosine similarity by default)\n",
        "if 'run' in glove_model.key_to_index:\n",
        "    top5_similar_words = glove_model.most_similar('run', topn=10)\n",
        "    for word, similarity in top5_similar_words:\n",
        "        print(f\"{word}: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5u582VcB_dz",
        "outputId": "adc60814-c1ce-43f4-d2d2-a79a9f57dbf4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runs: 0.7017062306404114\n",
            "running: 0.6978930830955505\n",
            "ran: 0.630337119102478\n",
            "go: 0.5583360195159912\n",
            "allowed: 0.551228940486908\n",
            "going: 0.5329922437667847\n",
            "went: 0.5286133289337158\n",
            "out: 0.5273594260215759\n",
            "start: 0.5214914083480835\n",
            "off: 0.510507345199585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Find top 10 most similar tokens (euclidean distance)\n",
        "if 'run' in glove_model.key_to_index:\n",
        "    happy_vector = glove_model['run']\n",
        "else:\n",
        "    raise ValueError(\"The word is not in the GloVe model's vocabulary.\")\n",
        "\n",
        "# calculate euclidean distance with all tokens\n",
        "distances = []\n",
        "for word in glove_model.key_to_index.keys():\n",
        "    word_vector = glove_model[word]\n",
        "    dist = np.linalg.norm(happy_vector - word_vector)\n",
        "    distances.append((word, dist))\n",
        "\n",
        "# sort and select top 10\n",
        "top10_similar_words = sorted(distances, key=lambda x: x[1])[1:11]\n",
        "\n",
        "for word, dist in top10_similar_words:\n",
        "    print(f\"{word}: {dist}\")\n"
      ],
      "metadata": {
        "id": "06iLDy1uISHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cd3837-c848-4f07-d2c5-6d171d0c9ad3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running: 4.71186637878418\n",
            "runs: 4.880781173706055\n",
            "ran: 5.099777698516846\n",
            "allowed: 5.4944844245910645\n",
            "go: 5.5153961181640625\n",
            "went: 5.581228256225586\n",
            "going: 5.697341442108154\n",
            "well: 5.726036071777344\n",
            "out: 5.763474941253662\n",
            "only: 5.772265434265137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText"
      ],
      "metadata": {
        "id": "kxxqUITULLkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "ft_model = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWXFVDZNLNNq",
        "outputId": "71269197-4738-4ae9-9908-2382396ead39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between 2 tokens\n",
        "similarity = ft_model.similarity('happy', 'joyful')\n",
        "print(\"Similarity between 'happy' and 'joyful':\", similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qDIS1sNLkGu",
        "outputId": "33e1cb09-720b-4715-9611-95ea82eb3caa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'happy' and 'joyful': 0.71287423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Find top 10 most similar tokens (cosine similarity by default)\n",
        "top10_words = ft_model.most_similar('run', topn=10)\n",
        "for word, similarity in top10_words:\n",
        "    print(f\"{word}: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whUVCmfaLXwZ",
        "outputId": "47d52dbd-59f7-4967-8791-3db469a6469e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running: 0.8197714686393738\n",
            "ran: 0.7697737216949463\n",
            "runnning: 0.7573951482772827\n",
            "runs: 0.7466383576393127\n",
            "trun: 0.7098832130432129\n",
            "runned: 0.7096849083900452\n",
            "retrun: 0.6958275437355042\n",
            "runnng: 0.6936571002006531\n",
            "runnign: 0.6931751370429993\n",
            "srun: 0.679145872592926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Find top 10 most similar tokens (euclidean distance)\n",
        "happy_vector = ft_model['run']\n",
        "word_distances = []\n",
        "\n",
        "# calculate euclidean distance with all tokens\n",
        "for word in ft_model.index_to_key:\n",
        "    word_vector = ft_model[word]\n",
        "    dist = np.linalg.norm(happy_vector - word_vector)\n",
        "    word_distances.append((word, dist))\n",
        "\n",
        "# sort and select top 10\n",
        "sorted_distances = sorted(word_distances, key=lambda x: x[1])[1:11]\n",
        "for word, dist in sorted_distances:\n",
        "    print(f\"{word}: {dist}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6J9__bBMw6G",
        "outputId": "7362bb66-e885-4bec-dc31-69341d356da9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running: 0.8109013438224792\n",
            "ran: 0.8868503570556641\n",
            "runs: 0.8995624780654907\n",
            "runned: 0.919409990310669\n",
            "trun: 0.92112797498703\n",
            "runnning: 0.9247350096702576\n",
            "retrun: 0.9549537301063538\n",
            "runnign: 0.9586350321769714\n",
            "run.: 0.9668093323707581\n",
            "runing: 0.9748907089233398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find top10 emojis:\n",
        "- cosine similarity + word2vec + emoji2vec\n",
        "- cosine similarity + word2vec + emojional\n",
        "- euclidean distance + word2vec + emoji2vec\n",
        "- euclidean distance + word2vec + emojional"
      ],
      "metadata": {
        "id": "-YxrGa98DRtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the emoji2vec emoji embeddings\n",
        "e2v = KeyedVectors.load_word2vec_format(\"emoji2vec.bin\", binary=True)"
      ],
      "metadata": {
        "id": "3krkZVwJCPN4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the emojional emoji embeddings\n",
        "el2v = KeyedVectors.load_word2vec_format(\"emojional.bin\", binary=True)"
      ],
      "metadata": {
        "id": "w9L1Vi4AFzgN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 cosine similarity + word2vec + emoji2vec\n",
        "word_vector = w2v_model.get_vector(\"run\")\n",
        "top5_emojis = e2v.most_similar(word_vector, topn=10)\n",
        "for emoji, similarity in top5_emojis:\n",
        "    print(f\"{emoji}: {similarity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hagUDNR4Dwzg",
        "outputId": "4146b73f-bd68-412b-92f2-8891527ee7d5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÉ: 0.623569667339325\n",
            "üéΩ: 0.4897686839103699\n",
            "üèá: 0.3370935022830963\n",
            "üèÅ: 0.3312745988368988\n",
            "üö∂: 0.33033713698387146\n",
            "üí∏: 0.30279526114463806\n",
            "üö∑: 0.2999289035797119\n",
            "üíØ: 0.29605159163475037\n",
            "üí≠: 0.29440468549728394\n",
            "üé≥: 0.2800602614879608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2 cosine similarity + word2vec + emojional\n",
        "word_vector = w2v_model.get_vector(\"run\")\n",
        "top5_emojis = el2v.most_similar(word_vector, topn=10)\n",
        "for emoji, similarity in top5_emojis:\n",
        "    print(f\"{emoji}: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V50Rk-9tGAVS",
        "outputId": "764fa82a-4abf-4e95-e37c-abc761865b7c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÉ‚Äç‚ôÄÔ∏è: 0.08441895246505737\n",
            "üèÉ‚Äç‚ôÇÔ∏è: 0.0673506110906601\n",
            "üëû: 0.054681211709976196\n",
            "‚åõ: 0.0460536852478981\n",
            "üëü: 0.03200191259384155\n",
            "üèá: 0.03135138377547264\n",
            "üö•: 0.030578602105379105\n",
            "‚õ≥: 0.028483860194683075\n",
            "ü™£: 0.027112886309623718\n",
            "üë®‚Äçü¶Ω: 0.015860840678215027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 euclidean distance + word2vec + emoji2vec\n",
        "if 'happy' in w2v_model.key_to_index:\n",
        "    happy_vector = w2v_model['run']\n",
        "else:\n",
        "    raise ValueError(\"The word is not in the Word2Vec model's vocabulary.\")\n",
        "\n",
        "emoji_distances = []\n",
        "for emoji in e2v.key_to_index.keys():\n",
        "    emoji_vector = e2v[emoji]\n",
        "    # calculate euclidean distance\n",
        "    dist = np.linalg.norm(happy_vector - emoji_vector)\n",
        "    emoji_distances.append((emoji, dist))\n",
        "\n",
        "top10_emojis = sorted(emoji_distances, key=lambda x: x[1])[:10]\n",
        "\n",
        "for emoji, dist in top10_emojis:\n",
        "    print(f\"{emoji}: {dist}\")"
      ],
      "metadata": {
        "id": "dpaNQ7lgI9RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7954704c-83bf-4620-bff2-a886933a4a63"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÉ: 1.5988041162490845\n",
            "üéΩ: 1.7596166133880615\n",
            "üèá: 1.9267877340316772\n",
            "üèÅ: 1.932873010635376\n",
            "üö∂: 1.933851718902588\n",
            "üí∏: 1.9623844623565674\n",
            "üö∑: 1.9653302431106567\n",
            "üíØ: 1.9693076610565186\n",
            "üí≠: 1.9709948301315308\n",
            "üé≥: 1.9856284856796265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 euclidean distance + word2vec + emojional\n",
        "if 'happy' in w2v_model.key_to_index:\n",
        "    happy_vector = w2v_model['run']\n",
        "else:\n",
        "    raise ValueError(\"The word is not in the Word2Vec model's vocabulary.\")\n",
        "\n",
        "emoji_distances = []\n",
        "for emoji in el2v.key_to_index.keys():\n",
        "    emoji_vector = el2v[emoji]\n",
        "    # calculate euclidean distance\n",
        "    dist = np.linalg.norm(happy_vector - emoji_vector)\n",
        "    emoji_distances.append((emoji, dist))\n",
        "\n",
        "top10_emojis = sorted(emoji_distances, key=lambda x: x[1])[:10]\n",
        "\n",
        "for emoji, dist in top10_emojis:\n",
        "    print(f\"{emoji}: {dist}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY6r1vZpJBjT",
        "outputId": "32c9f6a7-5fac-4c7e-930d-790bad0d8ac7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üë±‚Äç‚ôÇÔ∏è: 3.6923322677612305\n",
            "üî≥: 4.122771263122559\n",
            "üë©‚Äçü¶±: 4.410493850708008\n",
            "ü§ô: 4.442147254943848\n",
            "üîò: 4.445608139038086\n",
            "üë®: 4.470482349395752\n",
            "üë±: 4.497052192687988\n",
            "üë®‚Äçü¶±: 4.6015167236328125\n",
            "üì±: 4.61079740524292\n",
            "üè°: 4.612793445587158\n"
          ]
        }
      ]
    }
  ]
}